
import random
import numpy as np
import pandas as pd
import math
import torch
from torchvision import datasets, transforms
from sklearn.model_selection import train_test_split
from tensorflow.keras.datasets import mnist
import seaborn as sns
import matplotlib.pyplot as plt
import time

np.random.seed(555)

def initialize_parameters(layer_dims):
    """
    This function initiate the weights and biases of the network.
    The initate is random and each od the weights get value : U(-1/m**0.5 , 1/m**0.5)
    while U is uniform distribution and m is the number of inputs in each layer.

    Input:
    an array of the dimensions of each layer in the network (layer 0 is the size of the flattened input, layer L is the output softmax).

    Output:
    a dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL).
    """
    init_parmeters = {}
    for layer in range(1,len(layer_dims)):
        count_b = int(layer_dims[layer])
        probablity_treshold = 1 / (np.sqrt(layer_dims[layer-1]))
        array_w = np.random.uniform(-probablity_treshold, probablity_treshold, size=(layer_dims[layer], layer_dims[layer-1]))
        array_b = np.zeros((count_b, 1))
        init_parmeters[layer] = (array_w , array_b)
    return init_parmeters

def linear_forward(A, W, b):
    """
   This function implements the linear part of a layer's forward propagation.
   The formula is : WtA + b

    input:
    A – the activations of the previous layer
    W – the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
    B – the bias vector of the current layer (of shape [size of current layer, 1])

    Output:
    Z – the linear component of the activation function (i.e., the value before applying the non-linear function)
    linear_cache – a dictionary containing A, W, b (stored for making the backpropagation easier to compute)

    """

    w_transpose = np.transpose(W)
    mult = np.dot(W, A)
    Z = np.add(mult, b)
    linear_cache = {}
    linear_cache["A"] = A
    linear_cache["W"] = W
    linear_cache["b"] = b
    return Z , linear_cache

def softmax(Z):
    """
    This function calculates the softmax of the last activation layer by the formula :
    exp(Zi)/sum(Zj).

    Input:
    Z – the linear component of the activation function

    Output:
    A – the activations of the layer
    activation_cache – returns Z, which will be useful for the backpropagation

    """
    Z_i = np.exp(Z)
    sum_Z = np.sum(Z_i, axis=0, keepdims=True)
    A = Z_i / sum_Z

    activation_cache = Z

    return A, activation_cache

def relu(Z):
    """
    This function calculates the relu of the activation for each layer by the formula:
    if x < 0: 0
    else : x

    Input:
    Z – the linear component of the activation function

    Output:
    A – the activations of the layer
    activation_cache – returns Z, which will be useful for the backpropagation

    """
    A = np.maximum(0, Z)
    activation_cache = Z
    return A, activation_cache

def linear_activation_forward(A_prev, W, B, activation):
    """
    This function implements the forward propagation for the LINEAR->ACTIVATION layer.

    Input:
    A_prev – activations of the previous layer
    W – the weights matrix of the current layer
    B – the bias vector of the current layer
    activation – a string, either “softmax” or “relu”

    Output:
    A – the activations of the current layer
    cache – a joint dictionary containing both linear_cache and activation_cache

    """
    new_z, linear_cache = linear_forward(A_prev, W, B)
    if activation == 'softmax':
        A , activation_cache = softmax(new_z)
    elif activation == 'relu':
        A, activation_cache = relu(new_z)
    cache = {'linear cache': linear_cache, 'activation cache': activation_cache}
    return A, cache

def L_model_forward(X, parameters, use_batchnorm = False):
    """
    This function implements forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation

    Input:
    X – the data, numpy array of shape (input size, number of examples)
    parameters – the initialized W and b parameters of each layer
    use_batchnorm - a boolean flag used to determine whether to apply batchnorm after the activation

    Output:
    AL – the last post-activation value
    caches – a list of all the cache objects generated by the linear_forward function

    """
    caches = []
    A_prev = X.copy()
    for layer in range(1, len(parameters)):
        w = parameters[layer][0]
        b = parameters[layer][1]
        A, cache = linear_activation_forward(A_prev, w, b, 'relu')
        A = apply_batchnorm(A) if use_batchnorm else A

        caches.append(cache)
        A_prev = A
    w = parameters[len(parameters)][0]
    b = parameters[len(parameters)][1]
    AL, cache = linear_activation_forward(A_prev, w, b, 'softmax')
    caches.append(cache)
    return AL, caches

def convert_to_one_hot(Y, num_classes):
    """
    This function get a true label vector for all smaples, and return matrix from shape (num_of_classes, number of samples).
    If the true label of sample i is 1, so the cell (1, i) contain 1, all the other cell (x, i) contain 0.

    Input:
    Y - Ground truth vector, shape: num_samples,1
    num_classes - number of possible classifications

    Output:
    A one hot encodding matrix of the shape (num_of_classes, number of examples)

    """
    one_hot = np.zeros((Y.shape[0], num_classes))
    one_hot[np.arange(Y.shape[0]), Y] = 1
    return one_hot

def compute_cost(AL, Y, L2 = 0, L2_weights = None):
    """
    This function implements the cost function defined by equation. The requested cost function is categorical cross-entropy loss.
    The formula is as follows :
    cost=-1/m*∑(1,m)∑(1,C)[yi*log⁡(y)].
    Where yi is one for the true class (“ground-truth”) and y ̂ is the softmax-adjusted prediction.

    Input:
    AL – probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
    Y – the labels vector (i.e. the ground truth)

    Output:
    cost – the cross-entropy cost

    """
    sum_of_weights = 0
    if L2 != 0:
      for w in L2_weights:
        sum_of_weights += np.sum(np.square(w))

    num_of_examples = AL.shape[1]
    loss = np.sum(Y * np.log(AL + 1e-8)) / (-num_of_examples) + L2 / 2 * (sum_of_weights)
    return loss

def apply_batchnorm(A):
    """
    This function performs batchnorm on the received activation values of a given layer.
    The formula is :
    mean = 1/m * sum (A)
    std**2 = 1/m * sum(A-mean)
    NA = A - meanT / (std**2 + 1e-8)

    Input:
    A - the activation values of a given layer

    Output:
    NA - the normalized activation values, based on the formula learned in class

    """
    mean = np.mean(A, axis=1, keepdims=True)
    std_square = np.std(A, axis=1, keepdims=True)
    NA = (A - mean) / np.sqrt(std_square + 1e-8)
    return NA


"""## Part 2 - backward"""

def Linear_backward(dZ, cache, L2 = 0):
    """
    This function implements the linear part of the backward propagation process for a single layer

    Input:
    dZ – the gradient of the cost with respect to the linear output of the current layer (layer l)
    cache – tuple of values (A_prev, W, b) coming from the forward propagation in the current layer

    Output:
    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW -- Gradient of the cost with respect to W (current layer l), same shape as W
    db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    A_prev = cache[0]
    m = A_prev.shape[1]
    W = cache[1]
    dB = np.mean(dZ, axis=1).reshape(dZ.shape[0],1)
    dW = (np.dot(dZ , np.transpose(A_prev)) + L2 * W) / m
    dA_prev = np.dot(np.transpose(W) , dZ) # CHANGED
    return dA_prev, dW, dB

def linear_activation_backward(dA, cache, activation, L2 = 0):
    """
    This function implements the backward propagation for the LINEAR->ACTIVATION layer.
    The function first computes dZ and then applies the linear_backward function.

    Input:
    dA – post activation gradient of the current layer
    cache – contains both the linear cache and the activations cache

    Output:
    dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW – Gradient of the cost with respect to W (current layer l), same shape as W
    db – Gradient of the cost with respect to b (current layer l), same shape as b

    """

    dZ = 0
    if (activation == 'relu'):
        dZ = relu_backward(dA, cache['activation cache'])
    elif (activation == 'softmax'):
        dZ = softmax_backward(dA, cache["Y"]) ## dA is AL - propbalilties vector, and cache is Y - the output vector
    linear_activation = (cache['linear cache']["A"],cache['linear cache']["W"],cache['linear cache']["b"])
    return Linear_backward(dZ, linear_activation, L2) # dA_prev, dW, dB

def relu_backward(dA, activation_cache):
    """
    This function implements backward propagation for a ReLU unit.
    The derivative of relu is :
    if x > 0: 1
    else: 0

    Input:
    dA – the post-activation gradient
    activation_cache – contains Z (stored during the forward propagation)

    Output:
    dZ – gradient of the cost with respect to Z

    """
    z = activation_cache
    z[z<=0] = 0
    z[z>0] = 1
    dZ = dA * z
    return dZ

def softmax_backward (dA, activation_cache):
    """
    This function implements backward propagation for a softmax unit.
    The derivative of the softmax function is: p_i-y_i, where p_i is the softmax-adjusted probability of the class and y_i is the “ground truth”
    (i.e. 1 for the real class, 0 for all others).

    Input:
    dA – the post-activation gradient
    (dA = output from softmax activation layer)
    activation_cache – contains Z (stored during the forward propagation)
    activation_cach = y , ground truth vector

    Output:
    dZ – gradient of the cost with respect to Z
    """
    return dA - activation_cache

def L_model_backward(AL, Y, caches, L2 = 0):
    """
    This function implements the backward propagation process for the entire network.

    The backpropagation for the softmax function should be done only once as only the output layers uses it and the RELU should be done iteratively over all the remaining layers of the network.

    Input:
    AL - the probabilities vector, the output of the forward propagation (L_model_forward)
    Y - the true labels vector (the "ground truth" - true classifications)
    Caches - list of caches containing for each layer: a) the linear cache; b) the activation cache

    Output:
    Grads - a dictionary with the gradients
                 grads["dA" + str(l)] = ...
                 grads["dW" + str(l)] = ...
                 grads["db" + str(l)] = ...

    """

    grads = {}
    l = len(caches)
    num_of_classes = AL.shape[0]
    caches[l-1]["Y"]= Y
    grads[f"dA{l}"], grads[f"dW{l}"], grads[f"db{l}"] = linear_activation_backward(AL, caches[l-1], 'softmax', L2)
    for layer in range(l - 1, 0, -1):
        grads[f"dA{layer}"], grads[f"dW{layer}"], grads[f"db{layer}"] = linear_activation_backward(grads[f"dA{layer + 1}"], caches[layer - 1], 'relu', L2)
    return grads

def Update_parameters(parameters, grads, learning_rate):
    """
    This function updates parameters using gradient descent

    Input:
    parameters – a python dictionary containing the DNN architecture’s parameters
    grads – a python dictionary containing the gradients (generated by L_model_backward)
    learning_rate – the learning rate used to update the parameters (the “alpha”)

    Output:
    parameters – the updated values of the parameters object provided as input

    """

    updated_params = {}
    for layer in range(1, len(parameters) + 1):
        a_dw = learning_rate * grads[f"dW{layer}"]
        a_db = learning_rate * grads[f"db{layer}"]
        w = parameters[layer][0] - a_dw
        b = parameters[layer][1] - a_db
        new_parameters = (w, b)
        updated_params[layer] = new_parameters
    return updated_params

"""## L LAYER MODEL"""

def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size, X_val = None, Y_val = None, epsilon = 0 , use_batchnorm =  False, stopping_critetion = 1, L2 = 0):
    """
    This function implements a L-layer neural network.

    Input:
    X , X_val – the train/val input data, a numpy array of shape (height*width , number_of_examples)
    Y , Y_val – the train/val  “real” labels of the data, a vector of shape (num_of_classes, number of examples)
    Layer_dims – a list containing the dimensions of each layer, including the input
    batch_size – the number of examples in a single training batch.
    epsilon - minimum required difference between the loss of the current holding and the previous holding in order to continue training.Default value 0.
    use_batchnorm - a boolean flag used to determine whether to apply batchnorm after the activation

    Output:
    parameters – the parameters learnt by the system during the training
    costs ,  val_costs – the values of the cost function of the train/ val
    counter - number of iterations actually performed

    """
    LOG_FILE_PATH_1 = f'costs-{batch_size}_{num_iterations}_{learning_rate}_{use_batchnorm}_{L2}.csv'
    LOG_FILE_PATH_2 = f'valcosts-{batch_size}_{num_iterations}_{learning_rate}_{use_batchnorm}_{L2}.csv'
    costs = []
    val_costs = []
    iters = []
    counter = 0
    parameters = initialize_parameters(layers_dims)
    num_samples = X.shape[1]
    iteration_per_epoch = math.ceil(num_samples / batch_size)
    batch_num = 0
    improvement_counter = 0
    for iter in range(num_iterations):
        if counter % iteration_per_epoch == 0:
            batch_num = 0
        if batch_num == iteration_per_epoch - 1:
            samples = X[: ,batch_num * batch_size :]
            true_labels = Y[: ,batch_num * batch_size :]
        else:
            samples = X[: ,batch_num * batch_size : (batch_num+1) * batch_size]
            true_labels = Y[: ,batch_num * batch_size : (batch_num+1) * batch_size]

        activation_softmax, caches = L_model_forward(samples, parameters, use_batchnorm)
        w_caches = []
        if L2 != 0:
          for cache in caches:
            w_caches.append(cache['linear cache']['W'])
        loss = compute_cost(activation_softmax, true_labels, L2=L2, L2_weights = w_caches)
        grads = L_model_backward(activation_softmax, true_labels, caches)
        parameters = Update_parameters(parameters, grads, learning_rate)
        if (counter % 100 == 0):
            costs.append(loss)
            if (X_val is None):
              continue
            else:
              AL_val, caches_val = L_model_forward(X_val, parameters, use_batchnorm = use_batchnorm)
              val_loss = compute_cost(AL_val, Y_val, L2, w_caches)
              val_costs.append(val_loss)
              iters.append(counter)
              print(f'Iteration {iter + 1}/{num_iterations}| train loss :{loss}| val loss :{val_loss}')
              # if (len(val_costs) > 2 and epsilon != None and val_costs[-2] - val_loss < epsilon): # stop running if val loss doesnt improve
              #     break
              if (len(val_costs) > 2 and epsilon != None and val_costs[-2] - val_loss < epsilon): # stop running if val loss doesnt improve
                  improvement_counter += 1
                  if improvement_counter == stopping_critetion:
                    break
              else:
                improvement_counter = 0

        # else:
        #     print(f'Iteration {iter + 1}/{num_iterations}| train loss :{loss}')
        counter += 1
        batch_num += 1
    # print(f'Total Epochs: {counter/iteration_per_epoch}')
    pd.DataFrame({'costs':costs, 'iters':iters}).to_csv(LOG_FILE_PATH_1, index=False)
    pd.DataFrame({'val_costs':val_costs, 'iters':iters}).to_csv(LOG_FILE_PATH_2, index=False)
    return parameters, costs, val_costs, counter

def Predict(X, Y, parameters):
    """"
    This function receives an input data and the true labels and calculates the accuracy of the trained neural network on the data.

    Input:
    X – the input data, a numpy array of shape (height*width, number_of_examples)
    Y – the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
    Parameters – a python dictionary containing the DNN architecture’s parameters

    Output:
    accuracy – the accuracy measure of the neural net on the provided datas
    """
    AL, caches = L_model_forward(X, parameters, use_batchnorm = False)
    predictions = np.argmax(AL, axis=0)
    true_labels = np.argmax(Y, axis=0)
    accuracy = np.sum(predictions == true_labels) / len(predictions)

    return accuracy

# part 3 - executions

def split_data(x_train, y_train, x_test, y_test, split = 0.2):
    """
    This is helper function that split the data into train and valdation and convert the Y vector into a metrix using the one hot encodding function.
    In addtion we normalize the data between 0 to 1 (by dividing in 255).

    Input:
    x_train / x_test -  the train/val input data, a numpy array of shape (height*width , number_of_examples)
    y_train / y_test -  the train/val  “real” labels of the data, a vector of shape (num_of_classes, number of examples)
    split: the split ratio default 0.2 for validation

    Output:
    x_train / x_val / x_test
    y_train / y_val / y_test

    """


    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=split, random_state=42)
    x_train = x_train.swapaxes(0,1) / 255.0
    x_val = x_val.swapaxes(0,1) / 255.0
    x_test = x_test.swapaxes(0,1) / 255.0

    y_train = convert_to_one_hot(y_train, 10).swapaxes(0,1)
    y_val = convert_to_one_hot(y_val, 10).swapaxes(0,1)
    y_test = convert_to_one_hot(y_test, 10).swapaxes(0,1)

    return x_train, x_val, y_train, y_val, x_test, y_test

"""## TRY"""

# 1. call MNIST data set
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 2. flat the data
x_train = x_train.reshape(x_train.shape[0],x_train.shape[1] * x_train.shape[2])
x_test = x_test.reshape(x_test.shape[0],x_test.shape[1] * x_test.shape[2])

# 3. split the data into train, test and validation
x_train, x_val, y_train, y_val, x_test, y_test = split_data(x_train, y_train, x_test, y_test, 0.2)

# 4.
layers_dims = [20,7,5,10]
learning_rate = 0.009

dim_num = x_train.shape[0]
layers_dims = [dim_num] + layers_dims

epsilon = 0.00001

def training_loop(hyperparams, stopping_critetion):
    """
    Executes training for multiple sets of hyperparameters, evaluates the performance, and saves the results.

    Parameters:
    hyperparams (dict): Dictionary of hyperparameters for each trial. Each key is a trial identifier and each value
                        is a dictionary with the following keys:
                        - "number_of_iterations": int, number of training iterations (epochs).
                        - "batch_size": int, size of the mini-batches used in training.
                        - "use_batchnorm": bool, whether to use batch normalization.
                        - "L2": int, size of the L2 regularization weight.
    stopping_critetion (float): Value for early stopping criteria to stop training if no improvement is seen.
    L2 (float): L2 regularization parameter to penalize large weights and reduce overfitting.
    """
    results = {}

    for trial in hyperparams.keys():
        # Extract hyperparameters for the current trial
        number_of_iterations = hyperparams[trial]["number_of_iterations"]
        batch_size = hyperparams[trial]["batchs_size"]
        use_batchnorm = hyperparams[trial]["use_batchnorm"]
        L2_w = hyperparams[trial]["L2"]

        # Print trial details
        print(f"Trial Details: Batch Size: {batch_size} No. Iterations: {number_of_iterations} Batchnorm: {use_batchnorm}")

        # Record start time
        start = time.time()

        # Train the model with the current set of hyperparameters
        parameters, costs, costs_val, iters = L_layer_model(
            x_train, y_train, layers_dims, learning_rate, number_of_iterations, batch_size,
            X_val=x_val, Y_val=y_val, epsilon=epsilon, use_batchnorm=use_batchnorm,
            stopping_critetion=stopping_critetion, L2=L2_w
        )
        results[trial] = parameters
        # Record end time
        end = time.time()

        # Evaluate the model
        train_acc = Predict(x_train, y_train, parameters)
        val_acc = Predict(x_val, y_val, parameters)
        test_acc = Predict(x_test, y_test, parameters)

        # Print results
        print(f"train loss: {costs[-1]} val loss: {costs_val[-1]} train_acc: {train_acc}, val_acc: {val_acc} test_acc: {test_acc} iters: {iters} training time: {end - start}")
    return results


hyperparams_test1 = {
                1: {"batchs_size" : 32 , "number_of_iterations" : 150000 , "use_batchnorm" : False, 'L2':0}, # 100 epochs
                2: {"batchs_size" : 64 , "number_of_iterations" : 75000 , "use_batchnorm" : False, 'L2':0}, # 100 epochs
                3: {"batchs_size" : 128 , "number_of_iterations" : 37500 , "use_batchnorm" : False, 'L2':0}, # 100 epochs
                4: {"batchs_size" : 256 , "number_of_iterations" : 18750 , "use_batchnorm" : False, 'L2':0}, # 100 epochs
              }

training_loop(hyperparams_test1, stopping_critetion = 1)


hyperparams_test2 = {
                1: {"batchs_size" : 32 , "number_of_iterations" : 150000 , "use_batchnorm" : False, 'L2':0}, # 100 epochs
                2: {"batchs_size" : 64 , "number_of_iterations" : 75000 , "use_batchnorm" : False, 'L2':0}, # 100 epochs
                3: {"batchs_size" : 128 , "number_of_iterations" : 37500 , "use_batchnorm" : False, 'L2':0}, # 100 epochs
                4: {"batchs_size" : 256 , "number_of_iterations" : 18750 , "use_batchnorm" : False, 'L2':0}, # 100 epochs
              }

training_loop(hyperparams_test2, stopping_critetion = 3)

hyperparams_test3 = {
                1: {"batchs_size" : 32 , "number_of_iterations" : 150000 , "use_batchnorm" : True, 'L2':0}, # 100 epochs
                2: {"batchs_size" : 64 , "number_of_iterations" : 75000 , "use_batchnorm" : True, 'L2':0}, # 100 epochs
                3: {"batchs_size" : 128 , "number_of_iterations" : 37500 , "use_batchnorm" : True, 'L2':0}, # 100 epochs
                4: {"batchs_size" : 256 , "number_of_iterations" : 18750 , "use_batchnorm" : True, 'L2':0}, # 100 epochs
              }

training_loop(hyperparams_test3, stopping_critetion = 3)

hyperparams_test4 = {
                1: {"batchs_size" : 32 , "number_of_iterations" : 150000 , "use_batchnorm" : True, 'L2':0}, # 100 epochs
                2: {"batchs_size" : 64 , "number_of_iterations" : 75000 , "use_batchnorm" : True, 'L2':0}, # 100 epochs
                3: {"batchs_size" : 128 , "number_of_iterations" : 37500 , "use_batchnorm" : True, 'L2':0}, # 100 epochs
                4: {"batchs_size" : 256 , "number_of_iterations" : 18750 , "use_batchnorm" : True, 'L2':0}, # 100 epochs
              }

training_loop(hyperparams_test4, stopping_critetion = 10)

hyperparams_test5 = {
                1: {"batchs_size" : 128 , "number_of_iterations" : 37500 , "use_batchnorm" : True, 'L2':0}, # 100 epochs
                2: {"batchs_size" : 128 , "number_of_iterations" : 37500 , "use_batchnorm" : False, 'L2':0}, # 100 epochs
                3: {"batchs_size" : 256 , "number_of_iterations" : 18750 , "use_batchnorm" : True, 'L2':0}, # 100 epochs
                4: {"batchs_size" : 256 , "number_of_iterations" : 18750 , "use_batchnorm" : False, 'L2':0}, # 100 epochs
              }

training_loop(hyperparams_test5, stopping_critetion = 5)

hyperparams_test6 = {
                1: {"batchs_size" : 256 , "number_of_iterations" : 18750 , "use_batchnorm" : False, "L2":0}, # 100 epochs
                2: {"batchs_size" : 256 , "number_of_iterations" : 18750 , "use_batchnorm" : False, "L2":0.01}, # 100 epochs
                3: {"batchs_size" : 256 , "number_of_iterations" : 18750 , "use_batchnorm" : False, "L2":0.05}, # 100 epochs
                4: {"batchs_size" : 256 , "number_of_iterations" : 18750 , "use_batchnorm" : True, "L2":0}, # 100 epochs
                5: {"batchs_size" : 256 , "number_of_iterations" : 18750 , "use_batchnorm" : True, "L2":0.01}, # 100 epochs
                6: {"batchs_size" : 256 , "number_of_iterations" : 18750 , "use_batchnorm" : True, "L2":0.05}, # 100 epochs
              }

training_loop(hyperparams_test6, stopping_critetion = 5)

